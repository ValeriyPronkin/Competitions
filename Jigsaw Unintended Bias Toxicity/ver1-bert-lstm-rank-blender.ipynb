{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for @christofhenkel @abhishek @iezepov for their great work:\n",
    "\n",
    "https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage\n",
    "https://www.kaggle.com/abhishek/pytorch-bert-inference\n",
    "https://www.kaggle.com/iezepov/starter-gensim-word-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "package_dir = \"../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\n",
    "sys.path.append(package_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import fastai\n",
    "from fastai.train import Learner\n",
    "from fastai.train import DataBunch\n",
    "from fastai.callbacks import *\n",
    "from fastai.basic_data import DatasetType\n",
    "import fastprogress\n",
    "from fastprogress import force_console_behavior\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "from keras.preprocessing import text, sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import torch.utils.data\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam\n",
    "from pytorch_pretrained_bert import BertConfig\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For BERT path\n",
    "def convert_lines(example, max_seq_length,tokenizer):\n",
    "    max_seq_length -=2\n",
    "    all_tokens = []\n",
    "    longer = 0\n",
    "    for text in tqdm(example):\n",
    "        tokens_a = tokenizer.tokenize(text)\n",
    "        if len(tokens_a)>max_seq_length:\n",
    "            tokens_a = tokens_a[:max_seq_length]\n",
    "            longer += 1\n",
    "        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n",
    "        all_tokens.append(one_token)\n",
    "    return np.array(all_tokens)\n",
    "\n",
    "def is_interactive():\n",
    "    return 'SHLVL' not in os.environ\n",
    "\n",
    "def seed_everything(seed=123):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "\n",
    "def load_embeddings(path):\n",
    "    #with open(path,'rb') as f:\n",
    "    emb_arr = KeyedVectors.load(path)\n",
    "    return emb_arr\n",
    "\n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((max_features + 1, 300))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i <= max_features:\n",
    "            try:\n",
    "                embedding_matrix[i] = embedding_index[word]\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    embedding_matrix[i] = embedding_index[word.lower()]\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        embedding_matrix[i] = embedding_index[word.title()]\n",
    "                    except KeyError:\n",
    "                        unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class SpatialDropout(nn.Dropout2d):\n",
    "    # –ú–µ—Ç–æ–¥ forward –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é \n",
    "    # –∑–∞–¥–∞–Ω–Ω–æ–π –Ω–µ–π—Ä–æ–π–Ω–Ω–æ–π —Å–µ—Ç–∏ –≤ –µ–µ –≤—ã—Ö–æ–¥—ã.\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)    # (N, T, 1, K)\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
    "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
    "        x = x.squeeze(2)  # (N, T, K)\n",
    "        return x\n",
    "\n",
    "def train_model(learn,test,output_dim,lr=0.001,\n",
    "                batch_size=512, n_epochs=4,\n",
    "                enable_checkpoint_ensemble=True):\n",
    "    \n",
    "    all_test_preds = []\n",
    "    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "    n = len(learn.data.train_dl)\n",
    "    phases = [(TrainingPhase(n).schedule_hp('lr', lr * (0.6**(i)))) for i in range(n_epochs)]\n",
    "    sched = GeneralScheduler(learn, phases)\n",
    "    learn.callbacks.append(sched)\n",
    "    for epoch in range(n_epochs):\n",
    "        learn.fit(1)\n",
    "        test_preds = np.zeros((len(test), output_dim))    \n",
    "        for i, x_batch in enumerate(test_loader):\n",
    "            X = x_batch[0].cuda()\n",
    "            y_pred = sigmoid(learn.model(X).detach().cpu().numpy())\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n",
    "\n",
    "        all_test_preds.append(test_preds)\n",
    "\n",
    "\n",
    "    if enable_checkpoint_ensemble:\n",
    "        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n",
    "    else:\n",
    "        test_preds = all_test_preds[-1]\n",
    "        \n",
    "    return test_preds\n",
    "\n",
    "def handle_punctuation(x):\n",
    "    x = x.translate(remove_dict)\n",
    "    x = x.translate(isolate_dict)\n",
    "    return x\n",
    "\n",
    "def handle_contractions(x):\n",
    "    x = tokenizer.tokenize(x)\n",
    "    return x\n",
    "\n",
    "def fix_quote(x):\n",
    "    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n",
    "    x = ' '.join(x)\n",
    "    return x\n",
    "\n",
    "def preprocess(x):\n",
    "    x = handle_punctuation(x)\n",
    "    x = handle_contractions(x)\n",
    "    x = fix_quote(x)\n",
    "    return x\n",
    "\n",
    "class SequenceBucketCollator():\n",
    "    def __init__(self, choose_length, sequence_index, length_index, label_index=None):\n",
    "        self.choose_length = choose_length\n",
    "        self.sequence_index = sequence_index\n",
    "        self.length_index = length_index\n",
    "        self.label_index = label_index\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        batch = [torch.stack(x) for x in list(zip(*batch))]\n",
    "        \n",
    "        sequences = batch[self.sequence_index]\n",
    "        lengths = batch[self.length_index]\n",
    "        \n",
    "        length = self.choose_length(lengths)\n",
    "        mask = torch.arange(start=maxlen, end=0, step=-1) < length\n",
    "        padded_sequences = sequences[:, mask]\n",
    "        \n",
    "        batch[self.sequence_index] = padded_sequences\n",
    "        \n",
    "        if self.label_index is not None:\n",
    "            return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\n",
    "    \n",
    "        return batch\n",
    "    \n",
    "    \n",
    "def custom_loss(data, targets):\n",
    "    bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:,1:2])(data[:,:1],targets[:,:1])\n",
    "    bce_loss_2 = nn.BCEWithLogitsLoss()(data[:,1:],targets[:,2:])\n",
    "    return (bce_loss_1 * loss_weight)*0.6 + bce_loss_2*0.4\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def ensemble_predictions(predictions, weights, type_=\"linear\"):\n",
    "    #Assert ‚Äî —ç—Ç–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è –ø—Ä–æ–≤–µ—Ä—è—Ç—å –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏—è –æ –∑–Ω–∞—á–µ–Ω–∏—è—Ö \n",
    "    #–ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–º –º–µ—Å—Ç–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã\n",
    "    assert np.isclose(np.sum(weights), 1.0)\n",
    "    #Returns a boolean array where two arrays are element-wise equal within a tolerance.\n",
    "    if type_ == \"linear\":\n",
    "        #–í—ã—á–∏—Å–ª–∏—Ç—å —Å—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–π –æ—Å–∏.\n",
    "        res = np.average(predictions, weights=weights, axis=0)\n",
    "    elif type_ == \"harmonic\":\n",
    "        res = np.average([1 / p for p in predictions], weights=weights, axis=0)\n",
    "        return 1 / res\n",
    "    elif type_ == \"geometric\":\n",
    "        numerator = np.average(\n",
    "            [np.log(p) for p in predictions], weights=weights, axis=0\n",
    "        )\n",
    "        res = np.exp(numerator / sum(weights))\n",
    "        return res\n",
    "    elif type_ == \"rank\":\n",
    "        res = np.average([rankdata(p) for p in predictions], weights=weights, axis=0)\n",
    "        return res / (len(res) + 1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    # –í–Ω—É—Ç—Ä–∏ —Ñ—É–Ω–∫—Ü–∏–∏ __init__ –º—ã –æ–±—ä—è–≤–ª—è–µ–º —Å–ª–æ–∏ –±—É–¥—É—â–µ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏.\n",
    "    def __init__(self, embedding_matrix, num_aux_targets):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        embed_size = embedding_matrix.shape[1]\n",
    "        \n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dropout = SpatialDropout(0.3)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "        #–ª–∏–Ω–µ–π–Ω—ã–µ —Å–ª–æ–∏ nn.Linear –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç –≤–∏–¥ W'x+b, –≥–¥–µ W ‚Äî –º–∞—Ç—Ä–∏—Ü–∞ –≤–µ—Å–æ–≤ —Ä–∞–∑–º–µ—Ä–æ–º \n",
    "        # (input, output) –∏ b ‚Äî –≤–µ–∫—Ç–æ—Ä —Å–º–µ—â–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–æ–º output. \n",
    "        #–≠—Ç–∏ —Å–∞–º—ã–µ –≤–µ—Å–∞ –∏ –±—É–¥—É—Ç \"–æ–±—É—á–∞—Ç—å—Å—è\" –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏.\n",
    "        #LSTM_UNITS = 128\n",
    "        # DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "        \n",
    "        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n",
    "        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n",
    "        #–í—ã—Ö–æ–¥–æ–≤ –∏–∑ —Å–µ—Ç–∫–∏ 8 - —Ç–∞—Ä–≥–µ—Ç + 7 identity + –≤–µ—Å\n",
    "    def forward(self, x, lengths=None):\n",
    "        h_embedding = self.embedding(x.long())\n",
    "        h_embedding = self.embedding_dropout(h_embedding)\n",
    "        \n",
    "        h_lstm1, _ = self.lstm1(h_embedding)\n",
    "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "        \n",
    "        # global average pooling\n",
    "        avg_pool = torch.mean(h_lstm2, 1)\n",
    "        # global max pooling\n",
    "        max_pool, _ = torch.max(h_lstm2, 1)\n",
    "        \n",
    "        h_conc = torch.cat((max_pool, avg_pool), 1)\n",
    "        h_conc_linear1  = F.relu(self.linear1(h_conc))\n",
    "        h_conc_linear2  = F.relu(self.linear2(h_conc))\n",
    "        \n",
    "        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
    "        \n",
    "        result = self.linear_out(hidden)\n",
    "        aux_result = self.linear_aux_out(hidden)\n",
    "        out = torch.cat([result, aux_result], 1)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.listdir('../input/pretext/bert1')\n",
    "#../input/pretext/BERT_apex_Epoch_1.bin\n",
    "#../input/pretext/bert1/bert_pytorch.bin\n",
    "#../input/pretext/bert1/bert_config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='once')\n",
    "device = torch.device('cuda')\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "SEED = 1234\n",
    "BATCH_SIZE = 512\n",
    "BERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "bert_config = BertConfig('../input/pretext/bert1/bert_config.json')\n",
    "#bert_config = BertConfig('../input/bert-inference/bert/bert_config.json')\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)\n",
    "\n",
    "tqdm.pandas()\n",
    "CRAWL_EMBEDDING_PATH = '../input/gensim-embeddings-dataset/crawl-300d-2M.gensim'\n",
    "GLOVE_EMBEDDING_PATH = '../input/gensim-embeddings-dataset/glove.840B.300d.gensim'\n",
    "#GOOGLE_EMBEDDING_PATH = '../input/gensim-embeddings-dataset/GoogleNews-vectors-negative300.gensim'\n",
    "#PARAGRAM_EMBEDDING_PATH = '../input/gensim-embeddings-dataset/paragram_300_sl999.gensim'\n",
    "NUM_MODELS = 2\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "MAX_LEN = 220\n",
    "if not is_interactive():\n",
    "    def nop(it, *a, **k):\n",
    "        return it\n",
    "\n",
    "    tqdm = nop\n",
    "\n",
    "    fastprogress.fastprogress.NO_BAR = True\n",
    "    master_bar, progress_bar = force_console_behavior()\n",
    "    fastai.basic_train.master_bar, fastai.basic_train.progress_bar = master_bar, progress_bar\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERT Part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\n",
    "test_df['comment_text'] = test_df['comment_text'].astype(str) \n",
    "X_test = convert_lines(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: '../input/pretext/bert.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-46107ea3c03e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/pretext/bert.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '../input/pretext/bert.bin'"
     ]
    }
   ],
   "source": [
    "os.listdir('../input/pretext/bert.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification(bert_config, num_labels=8)\n",
    "model.load_state_dict(torch.load('../input/pretext/bert.bin', map_location={'cuda:2':'cuda:0'}))\n",
    "model.to(device)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.zeros((len(X_test)))\n",
    "test = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=512, shuffle=False)\n",
    "tk0 = tqdm(test_loader)\n",
    "for i, (x_batch,) in enumerate(tk0):\n",
    "    pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n",
    "    test_preds[i * 512:(i + 1) * 512] = pred[:, 0].detach().cpu().squeeze().numpy()\n",
    "\n",
    "test_pred = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "submission = pd.read_csv(\n",
    "    \"../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv\"\n",
    ")\n",
    "submission[\"prediction\"] = test_pred\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "submission_bert = pd.DataFrame.from_dict({\n",
    "    'id': test_df['id'],\n",
    "    'prediction': test_pred\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM Part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 619.65 MB\n",
      "Memory usage after optimization is: 350.87 MB\n",
      "Decreased by 43.4%\n"
     ]
    }
   ],
   "source": [
    "train_df = reduce_mem_usage(pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_to_isolate = '.,?!-;*\"‚Ä¶:‚Äî()%#$&_/@Ôºº„Éªœâ+=‚Äù‚Äú[]^‚Äì>\\\\¬∞<~‚Ä¢‚â†‚Ñ¢Àà ä…í‚àû¬ß{}¬∑œÑŒ±‚ù§‚ò∫…°|¬¢‚ÜíÃ∂`‚ù•‚îÅ‚î£‚î´‚îóÔºØ‚ñ∫‚òÖ¬©‚Äï…™‚úî¬Æ\\x96\\x92‚óè¬£‚ô•‚û§¬¥¬π‚òï‚âà√∑‚ô°‚óê‚ïë‚ñ¨‚Ä≤…îÀê‚Ç¨€©€û‚Ä†Œº‚úí‚û•‚ïê‚òÜÀå‚óÑ¬Ω ªœÄŒ¥Œ∑ŒªœÉŒµœÅŒΩ É‚ú¨Ôº≥ÔºµÔº∞Ôº•Ôº≤Ôº©Ôº¥‚òª¬±‚ôç¬µ¬∫¬æ‚úì‚óæÿüÔºé‚¨Ö‚ÑÖ¬ª–í–∞–≤‚ù£‚ãÖ¬ø¬¨‚ô´Ôº£Ôº≠Œ≤‚ñà‚ñì‚ñí‚ñë‚áí‚≠ê‚Ä∫¬°‚ÇÇ‚ÇÉ‚ùß‚ñ∞‚ñî‚óû‚ñÄ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ÜôŒ≥ÃÑ‚Ä≥‚òπ‚û°¬´œÜ‚Öì‚Äû‚úãÔºö¬•Ã≤ÃÖÃÅ‚àô‚Äõ‚óá‚úè‚ñ∑‚ùì‚ùó¬∂ÀöÀôÔºâ—Å–∏ ø‚ú®„ÄÇ…ë\\x80‚óïÔºÅÔºÖ¬Ø‚àíÔ¨ÇÔ¨Å‚ÇÅ¬≤ å¬º‚Å¥‚ÅÑ‚ÇÑ‚å†‚ô≠‚úò‚ï™‚ñ∂‚ò≠‚ú≠‚ô™‚òî‚ò†‚ôÇ‚òÉ‚òé‚úà‚úå‚ú∞‚ùÜ‚òô‚óã‚Ä£‚öìÂπ¥‚àé‚Ñí‚ñ™‚ñô‚òè‚ÖõÔΩÉÔΩÅÔΩì«Ä‚ÑÆ¬∏ÔΩó‚Äö‚àº‚Äñ‚Ñ≥‚ùÑ‚Üê‚òº‚ãÜ í‚äÇ„ÄÅ‚Öî¬®Õ°‡πè‚öæ‚öΩŒ¶√óŒ∏Ôø¶ÔºüÔºà‚ÑÉ‚è©‚òÆ‚ö†Êúà‚úä‚ùå‚≠ï‚ñ∏‚ñ†‚áå‚òê‚òë‚ö°‚òÑ«´‚ï≠‚à©‚ïÆÔºå‰æãÔºû ï…êÃ£Œî‚ÇÄ‚úû‚îà‚ï±‚ï≤‚ñè‚ñï‚îÉ‚ï∞‚ñä‚ñã‚ïØ‚î≥‚îä‚â•‚òí‚Üë‚òù…π‚úÖ‚òõ‚ô©‚òûÔº°Ôº™Ôº¢‚óî‚ó°‚Üì‚ôÄ‚¨ÜÃ±‚Ñè\\x91‚†ÄÀ§‚ïö‚Ü∫‚á§‚àè‚úæ‚ó¶‚ô¨¬≥„ÅÆÔΩúÔºè‚àµ‚à¥‚àöŒ©¬§‚òú‚ñ≤‚Ü≥‚ñ´‚Äø‚¨á‚úßÔΩèÔΩñÔΩçÔºçÔºíÔºêÔºòÔºá‚Ä∞‚â§‚àïÀÜ‚öú‚òÅ'\n",
    "symbols_to_delete = '\\nüçï\\rüêµüòë\\xa0\\ue014\\t\\uf818\\uf04a\\xadüò¢üê∂Ô∏è\\uf0e0üòúüòéüëä\\u200b\\u200eüòÅÿπÿØŸàŸäŸáÿµŸÇÿ£ŸÜÿßÿÆŸÑŸâÿ®ŸÖÿ∫ÿ±üòçüíñüíµ–ïüëéüòÄüòÇ\\u202a\\u202cüî•üòÑüèªüí•·¥ç è Ä·¥á…¥·¥Ö·¥è·¥Ä·¥ã ú·¥ú ü·¥õ·¥Ñ·¥ò ô“ì·¥ä·¥°…¢üòãüëè◊©◊ú◊ï◊ù◊ë◊ôüò±‚Äº\\x81„Ç®„É≥„Ç∏ÊïÖÈöú\\u2009üöå·¥µÕûüåüüòäüò≥üòßüôÄüòêüòï\\u200füëçüòÆüòÉüòò◊ê◊¢◊õ◊óüí©üíØ‚õΩüöÑüèº‡Æúüòñ·¥†üö≤‚Äêüòüüòàüí™üôèüéØüåπüòáüíîüò°\\x7füëå·ºê·Ω∂ŒÆŒπ·Ω≤Œ∫·ºÄŒØ·øÉ·º¥ŒæüôÑÔº®üò†\\ufeff\\u2028üòâüò§‚õ∫üôÇ\\u3000ÿ™ÿ≠ŸÉÿ≥ÿ©üëÆüíôŸÅÿ≤ÿ∑üòèüçæüéâüòû\\u2008üèæüòÖüò≠üëªüò•üòîüòìüèΩüéÜüçªüçΩüé∂üå∫ü§îüò™\\x08‚Äëüê∞üêáüê±üôÜüò®üôÉüíïùòäùò¶ùò≥ùò¢ùòµùò∞ùò§ùò∫ùò¥ùò™ùòßùòÆùò£üíóüíöÂú∞ÁçÑË∞∑—É–ª–∫–Ω–ü–æ–ê–ùüêæüêïüòÜ◊îüîóüöΩÊ≠åËàû‰ºéüôàüò¥üèøü§óüá∫üá∏–ºœÖ—Ç—ï‚§µüèÜüéÉüò©\\u200aüå†üêüüí´üí∞üíé—ç–ø—Ä–¥\\x95üñêüôÖ‚õ≤üç∞ü§êüëÜüôå\\u2002üíõüôÅüëÄüôäüôâ\\u2004À¢·µí ≥ ∏·¥º·¥∑·¥∫ ∑·µó ∞·µâ·µò\\x13üö¨ü§ì\\ue602üòµŒ¨ŒøœåœÇŒ≠·Ω∏◊™◊û◊ì◊£◊†◊®◊ö◊¶◊òüòíÕùüÜïüëÖüë•üëÑüîÑüî§üëâüë§üë∂üë≤üîõüéì\\uf0b7\\uf04c\\x9f\\x10ÊàêÈÉΩüò£‚è∫üòåü§ëüåèüòØ–µ—Öüò≤·º∏·æ∂·ΩÅüíûüöìüîîüìöüèÄüëê\\u202düí§üçá\\ue613Â∞èÂúüË±Üüè°‚ùî‚Åâ\\u202füë†„Äã‡§ï‡§∞‡•ç‡§Æ‡§æüáπüáºüå∏Ëî°Ëã±Êñáüåûüé≤„É¨„ÇØ„Çµ„ÇπüòõÂ§ñÂõΩ‰∫∫ÂÖ≥Á≥ª–°–±üíãüíÄüéÑüíúü§¢ŸêŸé—å—ã–≥—è‰∏çÊòØ\\x9c\\x9düóë\\u2005üíÉüì£üëø‡ºº„Å§‡ºΩüò∞·∏∑–ó–∑‚ñ±—ÜÔøºü§£ÂçñÊ∏©Âì•ÂçéËÆÆ‰ºö‰∏ãÈôç‰Ω†Â§±ÂéªÊâÄÊúâÁöÑÈí±Âä†ÊãøÂ§ßÂùèÁ®éÈ™óÂ≠êüêù„ÉÑüéÖ\\x85üç∫ÿ¢ÿ•ÿ¥ÿ°üéµüåéÕü·ºîÊ≤πÂà´ÂÖãü§°ü§•üò¨ü§ß–π\\u2003üöÄü§¥ ≤—à—á–ò–û–†–§–î–Ø–ú—é–∂üòùüñë·Ωê·ΩªœçÁâπÊÆä‰ΩúÊà¶Áæ§—âüí®ÂúÜÊòéÂõ≠◊ß‚Ñêüèàüò∫üåç‚èè·ªáüçîüêÆüçÅüçÜüçëüåÆüåØü§¶\\u200dùìíùì≤ùìøùìµÏïàÏòÅÌïòÏÑ∏Ïöî–ñ—ô–ö—õüçÄüò´ü§§·ø¶ÊàëÂá∫ÁîüÂú®‰∫ÜÂèØ‰ª•ËØ¥ÊôÆÈÄöËØùÊ±âËØ≠Â•ΩÊûÅüéºüï∫üç∏ü•ÇüóΩüéáüéäüÜòü§†üë©üñíüö™Â§©‰∏ÄÂÆ∂‚ö≤\\u2006‚ö≠‚öÜ‚¨≠‚¨Ø‚èñÊñ∞‚úÄ‚ïåüá´üá∑üá©üá™üáÆüá¨üáßüò∑üá®üá¶–•–®üåê\\x1fÊùÄÈ∏°ÁªôÁå¥Áúã Åùó™ùóµùó≤ùóªùòÜùóºùòÇùóøùóÆùóπùó∂ùòáùóØùòÅùó∞ùòÄùòÖùóΩùòÑùó±üì∫œñ\\u2000“Ø’Ω·¥¶·é•“ªÕ∫\\u2007’∞\\u2001…©ÔΩôÔΩÖ‡µ¶ÔΩå∆ΩÔΩàùêìùê°ùêûùê´ùêÆùêùùêöùêÉùêúùê©ùê≠ùê¢ùê®ùêß∆Ñ·¥®◊ü·ëØ‡ªêŒ§·èß‡Ø¶–Ü·¥ë‹Åùê¨ùê∞ùê≤ùêõùê¶ùêØùêëùêôùê£ùêáùêÇùêòùüé‘ú–¢·óû‡±¶„Äî·é´ùê≥ùêîùê±ùüîùüìùêÖüêãÔ¨Éüíòüíì—ëùò•ùòØùò∂üíêüåãüåÑüåÖùô¨ùôñùô®ùô§ùô£ùô°ùôÆùôòùô†ùôöùôôùôúùôßùô•ùô©ùô™ùôóùôûùôùùôõüë∫üê∑‚ÑãùêÄùê•ùê™üö∂ùô¢·ºπü§òÕ¶üí∏ÿ¨Ìå®Ìã∞Ôº∑ùôá·µªüëÇüëÉ…úüé´\\uf0a7–ë–£—ñüö¢üöÇ‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä·øÜüèÉùì¨ùìªùì¥ùìÆùìΩùìº‚òòÔ¥æÃØÔ¥ø‚ÇΩ\\ue807ùëªùíÜùíçùíïùíâùíìùíñùíÇùíèùíÖùíîùíéùíóùíäüëΩüòô\\u200c–õ‚Äíüéæüëπ‚éåüèí‚õ∏ÂÖ¨ÂØìÂÖªÂÆ†Áâ©ÂêóüèÑüêÄüöëü§∑ÊìçÁæéùíëùíöùíêùë¥ü§ôüêíÊ¨¢ËøéÊù•Âà∞ÈòøÊãâÊñØ◊°◊§ùô´üêàùíåùôäùô≠ùôÜùôãùôçùòºùôÖÔ∑ªü¶ÑÂ∑®Êî∂Ëµ¢ÂæóÁôΩÈ¨ºÊÑ§ÊÄíË¶Å‰π∞È¢ù·∫Ωüöóüê≥ùüèùêüùüñùüëùüïùíÑùüóùê†ùôÑùôÉüëáÈîüÊñ§Êã∑ùó¢ùü≥ùü±ùü¨‚¶Å„Éû„É´„Éè„Éã„ÉÅ„É≠Ê†™ÂºèÁ§æ‚õ∑ÌïúÍµ≠Ïñ¥„Ñ∏„ÖìÎãàÕú ñùòøùôî‚Çµùí©‚ÑØùíæùìÅùí∂ùìâùìáùìäùìÉùìàùìÖ‚Ñ¥ùíªùíΩùìÄùìåùí∏ùìéùôèŒ∂ùôüùòÉùó∫ùüÆùü≠ùüØùü≤üëãü¶äÂ§ö‰º¶üêΩüéªüéπ‚õìüèπüç∑ü¶Ü‰∏∫Âíå‰∏≠ÂèãË∞äÁ•ùË¥∫‰∏éÂÖ∂ÊÉ≥Ë±°ÂØπÊ≥ïÂ¶ÇÁõ¥Êé•ÈóÆÁî®Ëá™Â∑±ÁåúÊú¨‰º†ÊïôÂ£´Ê≤°ÁßØÂîØËÆ§ËØÜÂü∫Áù£ÂæíÊõæÁªèËÆ©Áõ∏‰ø°ËÄ∂Á®£Â§çÊ¥ªÊ≠ªÊÄ™‰ªñ‰ΩÜÂΩì‰ª¨ËÅä‰∫õÊîøÊ≤ªÈ¢òÊó∂ÂÄôÊàòËÉúÂõ†Âú£ÊääÂÖ®Â†ÇÁªìÂ©öÂ≠©ÊÅêÊÉß‰∏îÊ†óË∞ìËøôÊ†∑Ëøò‚ôæüé∏ü§ïü§í‚õëüéÅÊâπÂà§Ê£ÄËÆ®üèùü¶Åüôãüò∂Ï•êÏä§ÌÉ±Ìä∏Î§ºÎèÑÏÑùÏú†Í∞ÄÍ≤©Ïù∏ÏÉÅÏù¥Í≤ΩÏ†úÌô©ÏùÑÎ†µÍ≤åÎßåÎì§ÏßÄÏïäÎ°ùÏûòÍ¥ÄÎ¶¨Ìï¥ÏïºÌï©Îã§Ï∫êÎÇòÏóêÏÑúÎåÄÎßàÏ¥àÏôÄÌôîÏïΩÍ∏àÏùòÌíàÎü∞ÏÑ±Î∂ÑÍ∞àÎïåÎäîÎ∞òÎìúÏãúÌóàÎêúÏÇ¨Ïö©üî´üëÅÂá∏·Ω∞üí≤üóØùôà·ºåùíáùíàùíòùíÉùë¨ùë∂ùïæùñôùñóùñÜùñéùñåùñçùñïùñäùñîùñëùñâùñìùñêùñúùñûùñöùñáùïøùñòùñÑùñõùñíùñãùñÇùï¥ùñüùñàùï∏üëëüöøüí°Áü•ÂΩºÁôæ\\uf005ùôÄùíõùë≤ùë≥ùëæùíãùüíüò¶ùôíùòæùòΩüèêùò©ùò®·Ωº·πëùë±ùëπùë´ùëµùë™üá∞üáµüëæ·ìá·íß·î≠·êÉ·êß·ê¶·ë≥·ê®·ìÉ·ìÇ·ë≤·ê∏·ë≠·ëé·ìÄ·ê£üêÑüéàüî®üêéü§ûüê∏üíüüé∞üåùüõ≥ÁÇπÂáªÊü•Áâàüç≠ùë•ùë¶ùëßÔºÆÔºßüë£\\uf020„Å£üèâ—Ñüí≠üé•Œûüê¥üë®ü§≥ü¶ç\\x0büç©ùëØùííüòóùüêüèÇüë≥üçóüïâüê≤⁄Ü€åùëÆùóïùó¥üçíÍú•‚≤£‚≤èüêë‚è∞ÈâÑ„É™‰∫ã‰ª∂—óüíä„Äå„Äç\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600ÁáªË£Ω„Ç∑ËôöÂÅΩÂ±ÅÁêÜÂ±à–ìùë©ùë∞ùíÄùë∫üå§ùó≥ùóúùóôùó¶ùóßüçä·Ω∫·ºà·º°œá·øñŒõ‚§èüá≥ùíôœà’Å’¥’•’º’°’µ’´’∂÷Ä÷Ç’§’±ÂÜ¨Ëá≥·ΩÄùíÅüîπü§öüçéùë∑üêÇüíÖùò¨ùò±ùò∏ùò∑ùòêùò≠ùòìùòñùòπùò≤ùò´⁄©Œíœéüí¢ŒúŒüŒùŒëŒïüá±‚ô≤ùùà‚Ü¥üíí‚äò»ªüö¥üñïüñ§ü•òüìçüëà‚ûïüö´üé®üåëüêªùêéùêçùêäùë≠ü§ñüééüòºüï∑ÔΩáÔΩíÔΩéÔΩîÔΩâÔΩÑÔΩïÔΩÜÔΩÇÔΩãùü∞üá¥üá≠üáªüá≤ùóûùó≠ùóòùó§üëºüìâüçüüç¶üåàüî≠„Ääüêäüêç\\uf10a·Éö⁄°üê¶\\U0001f92f\\U0001f92aüê°üí≥·º±üôáùó∏ùóüùó†ùó∑ü•ú„Åï„Çà„ÅÜ„Å™„Çâüîº'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\n",
    "remove_dict = {ord(c):f'' for c in symbols_to_delete}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1780823/1804874 [08:45<00:07, 3387.03it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97320/97320 [00:28<00:00, 3432.64it/s]\n"
     ]
    }
   ],
   "source": [
    "x_train = train_df['comment_text'].progress_apply(lambda x:preprocess(x))\n",
    "x_test = test_df['comment_text'].progress_apply(lambda x:preprocess(x))\n",
    "\n",
    "#identity_columns = [\n",
    "#    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "#    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "identity_columns = ['male','female','transgender','other_gender','heterosexual','homosexual_gay_or_lesbian','bisexual','other_sexual_orientation',\n",
    "                'christian','jewish','muslim','hindu','buddhist','atheist','other_religion','black','white','asian','latino','other_race_or_ethnicity',\n",
    "                'physical_disability','intellectual_or_learning_disability','psychiatric_or_mental_illness','other_disability']\n",
    "train_df['identity_sum']=(train_df[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int)\n",
    "y_aux_train = train_df[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat', 'identity_sum']]\n",
    "\n",
    "# Overall\n",
    "weights = np.ones((len(x_train),)) / 4\n",
    "# Subgroup\n",
    "weights += (train_df[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n",
    "# Background Positive, Subgroup Negative\n",
    "weights += (( (train_df['target'].values>=0.5).astype(bool).astype(np.int) +\n",
    "   (train_df[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "# Background Negative, Subgroup Positive\n",
    "weights += (( (train_df['target'].values<0.5).astype(bool).astype(np.int) +\n",
    "   (train_df[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "weights += ( (train_df['target'].values>=0.5).astype(bool).astype(np.int) + \n",
    "             (train_df['identity_sum'].fillna(0).values==1)).astype(bool).astype(np.int) *0.4\n",
    "loss_weight = 1.0 / weights.mean()\n",
    "\n",
    "y_train = np.vstack([(train_df['target'].values>=0.5).astype(np.int),weights]).T\n",
    "#np.vstack([train_df['target'],weights]).T # –º—è–≥–∫–∏–π —Ç–∞—Ä–≥–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ —É–ª—É—á—à–∏–ª\n",
    "\n",
    "max_features = 410047"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words = max_features, filters='',lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n unknown words (crawl):  155104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n unknown words (glove):  158732\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
    "\n",
    "crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\n",
    "print('n unknown words (crawl): ', len(unknown_words_crawl))\n",
    "\n",
    "glove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\n",
    "print('n unknown words (glove): ', len(unknown_words_glove))\n",
    "\n",
    "max_features = max_features or len(tokenizer.word_index) + 1\n",
    "max_features\n",
    "\n",
    "embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\n",
    "embedding_matrix.shape\n",
    "\n",
    "del crawl_matrix\n",
    "del glove_matrix\n",
    "gc.collect()\n",
    "\n",
    "y_train_torch = torch.tensor(np.hstack([y_train, y_aux_train]), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = torch.from_numpy(np.array([len(x) for x in x_train]))\n",
    " \n",
    "maxlen = 300\n",
    "x_train_padded = torch.from_numpy(sequence.pad_sequences(x_train, maxlen=maxlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lengths = torch.from_numpy(np.array([len(x) for x in x_test]))\n",
    "\n",
    "x_test_padded = torch.from_numpy(sequence.pad_sequences(x_test, maxlen=maxlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "test_dataset = data.TensorDataset(x_test_padded, test_lengths)\n",
    "train_dataset = data.TensorDataset(x_train_padded, lengths, y_train_torch)\n",
    "valid_dataset = data.Subset(train_dataset, indices=[0, 1])\n",
    "\n",
    "train_collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), \n",
    "                                        sequence_index=0, \n",
    "                                        length_index=1, \n",
    "                                        label_index=2)\n",
    "test_collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), sequence_index=0, length_index=1)\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_collator)\n",
    "valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=train_collator)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=test_collator)\n",
    "\n",
    "databunch = DataBunch(train_dl=train_loader, valid_dl=valid_loader, collate_fn=train_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  0\n",
      "epoch     train_loss  valid_loss  time    \n",
      "0         0.162711    0.003787    10:37     \n",
      "epoch     train_loss  valid_loss  time    \n",
      "0         0.161288    0.005092    10:36     \n",
      "epoch     train_loss  valid_loss  time    \n",
      "0         0.153414    0.004572    10:37     \n",
      "epoch     train_loss  valid_loss  time    \n",
      "0         0.148741    0.004770    10:39     \n",
      "Model  1\n",
      "epoch     train_loss  valid_loss  time    \n",
      "0         0.166068    0.003398    10:37     \n",
      "epoch     train_loss  valid_loss  time    \n",
      "0         0.156345    0.003577    10:40     \n",
      "epoch     train_loss  valid_loss  time    \n",
      "0         0.152902    0.003155    10:44     \n",
      "epoch     train_loss  valid_loss  time    \n",
      "0         0.148877    0.004783    10:47     \n"
     ]
    }
   ],
   "source": [
    "all_test_preds = []\n",
    "for model_idx in range(NUM_MODELS):\n",
    "    print('Model ', model_idx)\n",
    "    seed_everything(1 + model_idx)\n",
    "    model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\n",
    "    learn = Learner(databunch, model, loss_func=custom_loss)\n",
    "    test_preds = train_model(learn,test_dataset,output_dim=8) # Learn + predict    \n",
    "    all_test_preds.append(test_preds)\n",
    "    filename = 'NN_STMS'+str(model_idx)+'.pth'\n",
    "    torch.save(learn.model.state_dict(), filename) # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_lstm = pd.DataFrame.from_dict({\n",
    "    'id': test_df['id'],\n",
    "    'prediction': np.mean(all_test_preds, axis=0)[:, 0]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Blending part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\n",
    "    \"../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv\"\n",
    ")\n",
    "\n",
    "weights = [0.7, 0.3]\n",
    "submission[\"prediction\"] = ensemble_predictions(\n",
    "    [submission_bert.prediction.values, submission_lstm.prediction.values],\n",
    "    weights,\n",
    "    type_=\"rank\",\n",
    ")\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
